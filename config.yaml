# Configuration for the SRM RAG system

# --- Mode Selection ---
# Select the active mode: "low", "medium", or "high"
current_mode: "low"

# --- General Settings ---
docs_path: "docs"
index_path: "index"

# --- Model Configuration ---
# Change models from this single location - they will be used throughout the application
ollama_model: "phi3:3.8b"  # LLM model for answer generation (e.g., "phi3:3.8b", "phi3:14b", "mistral:latest")
                              # Note: Embedding and reranker models are configured per mode below

use_direct_results: false  # Set to true to return raw search results without LLM processing
strict_mode: true  # Set to true to enable hallucination detection and validation

# --- RAG Mode Configurations ---
modes:
  # Low mode: Optimized for low-spec CPU systems without GPU
  # Minimal resource usage while maintaining basic RAG functionality
  low:
    # Embedding model for semantic search (change here to use throughout processing/search)
    embedding_model: "all-MiniLM-L6-v2"  # Best balance of speed/quality for CPU
                                          # For very low-spec systems, consider: "all-distilroberta-v1"
    # Reranker model for result ranking
    reranker_model: "cross-encoder/ms-marco-MiniLM-L-2-v2"  # Lightweight reranker

    # Retrieval settings - optimized for low-spec systems
    top_k_bm25: 6        # Reduced from 8 for faster BM25 processing
    top_k_faiss: 6       # Reduced from 8 for faster vector search
    top_k_reranked: 4    # Reduced from 6 to minimize reranking overhead

    # Advanced features - minimal for performance
    enable_multi_query_generation: false  # Disabled - saves CPU cycles
    enable_reranking: false               # Disabled - heavy CPU operation
    enable_multi_stage_generation: false  # Disabled - not needed for low mode
    enable_diversity_selection: true      # Keep for basic quality
    enable_document_diversity: true       # Keep for multi-document results

    # Performance optimizations for low-spec systems
    batch_size: 16                        # Smaller batches for low memory
    max_concurrent_searches: 1            # Sequential processing
    embedding_cache_size: 100             # Limit memory usage

    # Context management - reduced for performance
    max_context_length: 4000              # Reduced context window
    max_context_length_simple: 6000       # Increased from 4000 for better simple queries
    max_context_length_medium: 8000       # Balanced for most custom queries
    max_context_length_complex: 10000     # For complex custom queries

  # Medium mode: Balanced speed and quality.
  # Enables reranking and other key features for better accuracy.
  medium:
    # Embedding model for semantic search
    embedding_model: "all-MiniLM-L6-v2"
    # Reranker model for result ranking (slightly better than low mode)
    reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
    
    # Retrieval settings
    top_k_bm25: 10
    top_k_faiss: 10
    top_k_reranked: 7
    
    # Advanced features
    enable_multi_query_generation: true
    enable_reranking: true
    enable_multi_stage_generation: true
    
    # Context management
    max_context_length_simple: 8000
    max_context_length_medium: 10000
    max_context_length_complex: 14000

  # High mode: Best quality, slowest response.
  # Uses more candidates for retrieval and reranking for maximum thoroughness.
  high:
    # Embedding model for semantic search (consider upgrading to a more powerful model)
    embedding_model: "all-MiniLM-L6-v2"  # Options: "all-mpnet-base-v2", "multi-qa-mpnet-base-dot-v1"
    # Reranker model for result ranking
    reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
    
    # Retrieval settings
    top_k_bm25: 12
    top_k_faiss: 12
    top_k_reranked: 7
    
    # Advanced features (all enabled)
    enable_multi_query_generation: true
    enable_reranking: true
    enable_multi_stage_generation: true
    
    # Context management
    max_context_length_simple: 8000
    max_context_length_medium: 10000
    max_context_length_complex: 16000
